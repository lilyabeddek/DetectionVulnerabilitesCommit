{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cae829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import bisect\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802374ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chunks(l, n):\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a322201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractInfos(f) :\n",
    "  comment,repository,commit,commitType=\"\",\"\",\"\",\"\"\n",
    "\n",
    "  with open(file, \"r\",encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "          #extract the commit information from the first line\n",
    "          if i==0:\n",
    "            infos=line.split(',')\n",
    "            repository=infos[0]\n",
    "            commit=infos[4]\n",
    "            commitType=infos[3]\n",
    "            comment= infos[-1:][0]\n",
    "          else:\n",
    "            if (\"diff --git\" in line) or (\"Signed-off-by\" in line) or (\"Reported-by\" in line) or (\"Acked-by\" in line):\n",
    "              break\n",
    "            else:\n",
    "              if line !=\"\\n\":\n",
    "                comment += line\n",
    "  return (repository,commit,commitType,comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bd629c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_fichiers(file):\n",
    "  liste=[]\n",
    "  repository=\"\"\n",
    "  commit=\"\"\n",
    "  commitType=\"\"\n",
    "  newFileName=\"\"\n",
    "  comment=\"\"\n",
    "\n",
    "  with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        repository,commit,commitType,comment= extractInfos(file)\n",
    "        for i, line in enumerate(f):\n",
    "          if \"diff --git\" in line:\n",
    "            newFileName=line.rsplit('/', 1)[1]\n",
    "            newFileLine=i\n",
    "            if newFileName != \"\" :\n",
    "              liste.append((f.name,repository,commit,commitType,newFileLine,newFileName[:-1],comment))\n",
    "\n",
    "  return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b912e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_that_contain(file,string):\n",
    "  with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n",
    "    return [(i,line) for i,line in enumerate(fp) if string in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee55402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Chars(word):\n",
    "    return [char for char in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2ff36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the method's code\n",
    "def process_file(filename, line_num):\n",
    "  \n",
    "    code = \"\"\n",
    "    cnt_braket = 0\n",
    "    cnt_parentheses=0\n",
    "    found_start = False\n",
    "    found_end = False\n",
    "\n",
    "    startbraket=0\n",
    "    startbraket2=0\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if(i >= line_num):\n",
    "              code += line\n",
    "              if line.count(\"(\") > 0:\n",
    "                cnt_parentheses += line.count(\"(\")\n",
    "               \n",
    "              if line.count(\")\") > 0:\n",
    "                cnt_parentheses -= line.count(\")\")\n",
    "                \n",
    "              if cnt_parentheses == 0 :\n",
    "                  startbraket=i\n",
    "                  break\n",
    "                \n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if(i >= startbraket):\n",
    "              \n",
    "              code += line\n",
    "              test=False\n",
    "            \n",
    "              if line == startbraket :\n",
    "                test= line.count(\"{\")>0  \n",
    "              else:\n",
    "                if line !=\"\" and line !=\"\\n\":  \n",
    "                  if split_Chars(line)[0]==\"+\" or  split_Chars(line)[0]==\"-\" :\n",
    "                    if len(split_Chars(line)) >1 : test= split_Chars(line)[1]==\"{\"\n",
    "                  else :\n",
    "                    test= split_Chars(line)[0]==\"{\"\n",
    "                    \n",
    "              if test:       \n",
    "                    found_start = True\n",
    "                    cnt_braket += line.count(\"{\")\n",
    "                    startbraket2=i\n",
    "                    break\n",
    "              else :\n",
    "               if line !=\"\" and line !=\"\\n\" and i!=startbraket:\n",
    "                 return \"\"\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        if found_start == True:\n",
    "          \n",
    "          for i, line in enumerate(f):\n",
    "              if(i > startbraket2):\n",
    "                code += line\n",
    "                if line.count(\"{\") > 0:\n",
    "                  cnt_braket += line.count(\"{\")\n",
    "                if line.count(\"}\") > 0:\n",
    "                  cnt_braket -= line.count(\"}\")\n",
    "                if cnt_braket ==0 and found_start:\n",
    "                  found_end = True\n",
    "                  return code\n",
    "    return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51a480bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e37fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign directory\n",
    "directory = 'D:\\PFEMaster\\Dataset et pretraitement\\Dataset_VCCPlus'\n",
    " \n",
    "# iterate over files in that directory\n",
    "files = Path(directory).glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cea2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialiser les variables\n",
    "fct_code=[]\n",
    "fct_names=[]\n",
    "tableRef=[]\n",
    "classification=[]\n",
    "embeddings_same_size=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fef83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    code=\"\"\n",
    "    #get the code without the annotations\n",
    "    with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n",
    "        for line in fp:\n",
    "            if (line[0]==\"+\" or line[0]==\"-\"):\n",
    "                code+=line[1:]\n",
    "            else:\n",
    "              code+=line\n",
    "            \n",
    "    #get the modified methods' names\n",
    "    fct_names= re.findall(r'(?<=(?<=int\\s)|(?<=void\\s)|(?<=string\\s)|(?<=double\\s)|(?<=float\\s)|(?<=\\w\\s)|(?<=char\\s)).*?(?=\\s?\\()', code)\n",
    "    fct_names=list(dict.fromkeys(fct_names))\n",
    "    \n",
    "    #get a list of the modified files\n",
    "    fichiers_modif=liste_fichiers(file)\n",
    "    position_fichiers=[f[4] for f in fichiers_modif]\n",
    "    \n",
    "    #get the  modified methods' code\n",
    "    for function in fct_names:\n",
    "      for line in lines_that_contain(file,function):\n",
    "        code = process_file(file,line[0])\n",
    "        if code !=\"\":\n",
    "            bisect.insort(position_fichiers, line[0])\n",
    "            index_fichier= position_fichiers[position_fichiers.index(line[0])-1]\n",
    "            position_fichiers.remove(line[0])\n",
    "\n",
    "            infos=[f for f in fichiers_modif if f[4]==index_fichier][0]\n",
    "            fct_code.append((infos[0],infos[1],infos[2],infos[3],infos[4],infos[5],line[0],function,code,infos[6]))\n",
    "\n",
    "    fct_names.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58b8e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokenize the methods\n",
    "for f in fct_code :\n",
    "   # Tokenization\n",
    "   nl_tokens=tokenizer.tokenize(f[9])\n",
    "   code_tokens=tokenizer.tokenize(f[8])\n",
    "   tokens=nl_tokens+[tokenizer.sep_token]+code_tokens\n",
    "\n",
    "   #verifier que la serie ne depasse pas 510\n",
    "   if len(tokens) > 510:\n",
    "      fractions = list(divide_chunks(code_tokens,509-len(nl_tokens)))\n",
    "      for fract in fractions:\n",
    "          fract= [tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+fract+[tokenizer.sep_token]\n",
    "          tokens_ids=tokenizer.convert_tokens_to_ids(fract)\n",
    "          tableRef.append((f[0],f[1],f[2],f[3],f[4],f[5],tokens_ids))\n",
    "   else:   \n",
    "      tokens= [tokenizer.cls_token]+tokens+[tokenizer.sep_token]\n",
    "      # Convert tokens to ids\n",
    "      tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "      tableRef.append((f[0],f[1],f[2],f[3],f[4],f[5],tokens_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6900c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Tokens/CodeBert/CodeBert_tableTokens_methodes.csv', \"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(tableRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e8abb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_value=max([len(embed[6]) for embed in tableRef ]) \n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ff8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "iter=0\n",
    "with open(\"D:/Embeddings/CodeBert/CodeBert_methodesEmbeddings.csv\", \"w\") as f:\n",
    "    for embed in tableRef :\n",
    "\n",
    "      tokens_same_size=np.pad(embed[6], (0, max_value-len(embed[6])), 'constant')\n",
    "      context_embeddings=model(torch.tensor(tokens_same_size).unsqueeze(0))\n",
    "      t = torch.flatten(context_embeddings.pooler_output)\n",
    "      arr=torch.Tensor.numpy(t.detach())\n",
    "        \n",
    "      wr = csv.writer(f)\n",
    "      wr.writerow(arr)\n",
    "      iter+=1\n",
    "      print(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca936f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
