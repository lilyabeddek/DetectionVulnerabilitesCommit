{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def divide_chunks(l, n):\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a8a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractInfos(f) :\n",
    "  comment,repository,commit,commitType=\"\",\"\",\"\",\"\"\n",
    "\n",
    "  with open(file, \"r\",encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "          #extract the commit information from the first line\n",
    "          if i==0:\n",
    "            infos=line.split(',')\n",
    "            repository=infos[0]\n",
    "            commit=infos[4]\n",
    "            commitType=infos[3]\n",
    "            comment= infos[-1:][0]\n",
    "          else:\n",
    "            if (\"diff --git\" in line) or (\"Signed-off-by\" in line) or (\"Reported-by\" in line) or (\"Acked-by\" in line):\n",
    "              break\n",
    "            else:\n",
    "              if line !=\"\\n\":\n",
    "                comment += line\n",
    "  return (repository,commit,commitType,comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dba24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69107586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assign directory\n",
    "directory = 'D:\\PFEMaster\\Dataset et pretraitement\\Dataset_VCCPlus'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1142ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # iterate over files in that directory\n",
    "files = Path(directory).glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e2998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of variables\n",
    "iter=0\n",
    "code=\"\"\n",
    "newFileName=\"\"\n",
    "newFileLine=0\n",
    "repository=\"\"\n",
    "commit=\"\"\n",
    "commitType=\"\"\n",
    "comment =\"\"\n",
    "tableRef=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921b3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate the commit files to extract the modified files\n",
    "for file in files:\n",
    "        repository,commit,commitType,comment= extractInfos(file)\n",
    "        with open(file, \"r\",encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                  #new modified file by the commit\n",
    "                  if \"diff --git\" in line:\n",
    "                        if newFileName != \"\" and code!= \"\" :\n",
    "                              extension = newFileName[-3:][:2]\n",
    "\n",
    "                              #do not include .h files\n",
    "                              if (extension != \".h\"):\n",
    "\n",
    "                                    # Tokenization\n",
    "                                    nl_tokens=tokenizer.tokenize(comment)\n",
    "                                    code_tokens=tokenizer.tokenize(code)\n",
    "\n",
    "                                    tokens=nl_tokens+[tokenizer.sep_token]+code_tokens\n",
    "\n",
    "                                    #check that the series does not exceed 510\n",
    "                                    if len(tokens) > 510:\n",
    "\n",
    "                                      fractions = list(divide_chunks(code_tokens,509-len(nl_tokens)))\n",
    "                                      for fract in fractions:\n",
    "                                        fract= [tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+fract+[tokenizer.sep_token]\n",
    "                                        tokens_ids=tokenizer.convert_tokens_to_ids(fract)\n",
    "                                        tableRef.append((f.name,repository,commit,commitType,newFileLine,newFileName[:-1],tokens_ids))\n",
    "\n",
    "                                    else:\n",
    "                                      tokens= [tokenizer.cls_token]+tokens+[tokenizer.sep_token]\n",
    "                                      # Convert tokens to ids\n",
    "                                      tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "                                      tableRef.append((f.name,repository,commit,commitType,newFileLine,newFileName[:-1],tokens_ids))\n",
    "                                    \n",
    "\n",
    "\n",
    "                        newFileName=line.rsplit('/', 1)[1]\n",
    "                        newFileLine=i\n",
    "                        code=\"\"\n",
    "\n",
    "                  else:\n",
    "                    code+=line\n",
    "\n",
    "\n",
    "            extension = newFileName[-3:][:2]\n",
    "            if (extension != \".h\" and code!= \"\"):\n",
    "\n",
    "                  # Tokenization\n",
    "                  nl_tokens=tokenizer.tokenize(comment)\n",
    "                  code_tokens=tokenizer.tokenize(code)\n",
    "\n",
    "                  tokens=nl_tokens+[tokenizer.sep_token]+code_tokens\n",
    "\n",
    "                  #check that the series does not exceed 510\n",
    "\n",
    "                  if len(tokens) > 510:\n",
    "\n",
    "                          fractions = list(divide_chunks(code_tokens,509-len(nl_tokens)))\n",
    "                          for fract in fractions:\n",
    "                            fract= [tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+fract+[tokenizer.sep_token]\n",
    "                            tokens_ids=tokenizer.convert_tokens_to_ids(fract)\n",
    "                            tableRef.append((f.name,repository,commit,commitType,newFileLine,newFileName[:-1],tokens_ids))\n",
    "                  else:\n",
    "\n",
    "                          tokens= [tokenizer.cls_token]+tokens+[tokenizer.sep_token]\n",
    "\n",
    "                          # Convert tokens to ids\n",
    "                          tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "                          #embeddings.append(tokens_ids)\n",
    "                          tableRef.append((f.name,repository,commit,commitType,newFileLine,newFileName[:-1],tokens_ids))\n",
    "\n",
    "\n",
    "            newFileName=\"\"\n",
    "            newFileLine=0\n",
    "            code=\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d62d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Tokens/CodeBert/CodeBert_tableTokens_files.csv', \"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(tableRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculate the maximum length of the series of tokens generated from the inputs\n",
    "max_value=max([len(serieTokens[6]) for serieTokens in tableRef ]) \n",
    "\n",
    "with open(\"D:/Embeddings/CodeBert/CodeBert_filesEmbeddings.csv\", \"w\") as f:\n",
    "    for serieTokens in tableRef :\n",
    "        \n",
    "          #Unify the length of the series of tokens\n",
    "          tokens_same_size=np.pad(serieTokens[6], (0, max_value-len(serieTokens[6])), 'constant')\n",
    "\n",
    "          context_embeddings=model(torch.tensor(tokens_same_size).unsqueeze(0))\n",
    "          t = torch.flatten(context_embeddings.pooler_output)\n",
    "          arr=torch.Tensor.numpy(t.detach()) #the embedding vector\n",
    "         \n",
    "          wr = csv.writer(f) #save the vector in a file\n",
    "          wr.writerow(arr)\n",
    "          \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
